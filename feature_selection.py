import numpy as np
import pandas as pd
import sys
import warnings
import attr_classes

from sklearn.feature_selection import chi2, f_classif
from category_encoders import TargetEncoder
from category_encoders import JamesSteinEncoder
from category_encoders import CatBoostEncoder
from category_encoders import CountEncoder
from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer

# Results display options
warnings.simplefilter(action='ignore', category=FutureWarning)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.expand_frame_repr', False)
pd.options.mode.chained_assignment = None


# Encoding choices
ENCODINGS = ['target', 'js', 'catboost', 'freq']
TARGET_ENCODINGS = ['target', 'js', 'catboost']

if len(sys.argv) < 3:
    sys.exit(
        'Arguments needed!\n'
        f'python3 {sys.argv[0]} path/to/dataset encoding\n'
        f"encoding: {' '.join(ENCODINGS)}\n"
        f'Example: python3 {sys.argv[0]} path/to/dataset.csv catboost'
    )
dataset_path = sys.argv[1]
encoding = sys.argv[2]

ENCODER_MAP = {
    'target': TargetEncoder,
    'js': JamesSteinEncoder,
    'catboost': CatBoostEncoder,
    'freq': CountEncoder
}
TARGET = 'HasDetections'


def chi_table(df, columns, target, title=None, is_print=False):
    vectors = df[columns]
    target = df[target]

    res_columns = ['chi_val', 'chi_p', 'chi_p_good']
    res_df = pd.DataFrame(index=columns, columns=res_columns)

    res_df['chi_val'], res_df['chi_p'] = chi2(vectors, target)
    res_df['chi_p_good'] = res_df['chi_p'] < 0.05

    res_df = res_df.sort_values('chi_val', ascending=False)
    if is_print:
        print()
        print(title)
        print(res_df)
    return res_df


def f_table(df, columns, target, title=None, is_print=True):
    vectors = df[columns]
    target = df[target]

    res_columns = ['f_val', 'f_p', 'f_p_good']
    res_df = pd.DataFrame(index=columns, columns=res_columns)

    res_df['f_val'], res_df['f_p'] = f_classif(vectors, target)
    res_df['f_p_good'] = res_df['f_p'] < 0.05

    res_df = res_df.sort_values('f_val', ascending=False)
    if is_print:
        print()
        print(title)
        print(res_df)
    return res_df


def corr_pairs(df, target, is_print):
    df = df.drop([target], axis=1)
    corr_matrix = df.corr().abs()

    all_high_corr_indices = np.where(corr_matrix > 0.9)
    high_corr_indices = [(corr_matrix.index[x], corr_matrix.columns[y])
                         for x, y in zip(*all_high_corr_indices)
                         if x != y and x < y]
    corr_s = corr_matrix.unstack()
    corr_s = corr_s[high_corr_indices]
    if is_print and len(corr_s) > 0:
        print()
        print('Highly Correlated Columns')
        print(corr_s)

    return high_corr_indices


def non_target_encode(encoder_class, df, columns):
    encoder = encoder_class()
    return encoder.fit_transform(df[columns])


def target_encode(encoder_class, df, columns, target):
    encoder = encoder_class()
    return encoder.fit_transform(df[columns], df[target])


def encode_all_cat_to_cont(df, encoding, columns, target):
    # Encode
    encoder_class = ENCODER_MAP[encoding]
    if encoding in TARGET_ENCODINGS:
        df[columns] = target_encode(
            encoder_class, df, columns, target
        )
    else:
        df[columns] = non_target_encode(
            encoder_class, df, columns
        )
    return df


# Read Data
df = pd.read_csv(dataset_path)

# -- DROPPING USELESS ATTRS --
# Drop ID column
df = df.drop(['MachineIdentifier'], axis=1)

# Drop columns with null > 50%
value_col = 'Null %'
mn_df = pd.DataFrame(columns=[value_col])
for attr in df.columns[:-1]:
    null_pct = df[attr].isna().sum() / len(df[attr]) * 100
    if null_pct > 50:
        mn_df.loc[attr, value_col] = null_pct
print()
print('Attributes with Majority Nulls')
print(mn_df)
df = df.drop(mn_df.index, axis=1)

# Drop columns that are dominated by single value
value_col = 'Single value domination %'
dom_df = pd.DataFrame(columns=[value_col])
for attr in df.columns[:-1]:
    dom_pct = df[attr].value_counts().iloc[0] / len(df[attr]) * 100
    if dom_pct > 95:
        dom_df.loc[attr, value_col] = dom_pct

print()
print('Attributes that dominated by a single value (>95%)')
print(dom_df)
df = df.drop(dom_df.index, axis=1)

# Assign attr class of remaining columns
nomi_cat_columns = np.intersect1d(attr_classes.NOMI_CAT_COLUMNS, df.columns)
nume_cat_columns = np.intersect1d(attr_classes.NUME_CAT_COLUMNS, df.columns)
boolean_columns = np.intersect1d(attr_classes.BOOLEAN_COLUMNS, df.columns)
cont_columns = np.intersect1d(attr_classes.CONT_COLUMNS, df.columns)

nb_cat_columns = np.union1d(nomi_cat_columns, nume_cat_columns)
cat_columns = np.union1d(nb_cat_columns, boolean_columns)

# Data Cleansing
# Convert non-boolean cat cols to string and lower the case
df[nb_cat_columns] = df[nb_cat_columns].fillna('temporary_nan_string')
df[nb_cat_columns] = df[nb_cat_columns].astype(str)
df[nb_cat_columns] = df[nb_cat_columns].apply(lambda x: x.str.lower())
df[nb_cat_columns] = df[nb_cat_columns].replace('temporary_nan_string',
                                                np.nan)

# -- DROPPING IRRELEVANT CATEGORICAL ATTRS --
# Use a copy because we want the original values intact for next step
dica_df = df.copy()

# Ordinal encode cat attrs
dica_df[nb_cat_columns] = non_target_encode(
    OrdinalEncoder, dica_df, nb_cat_columns
)

# Impute missing values for boolean columns
imputer = SimpleImputer()
dica_df[boolean_columns] = imputer.fit_transform(dica_df[boolean_columns])

# Remove irrelevant cat attrs using chi table
chi_df = chi_table(dica_df, cat_columns, TARGET,
                   'Categorical Attributes', False)
irr_cat_columns = chi_df.index[chi_df['chi_p_good'] == False]
if (len(irr_cat_columns) > 0):
    print()
    print('Irrelevant Categorical Columns')
    print(chi_df.loc[irr_cat_columns, :])
df = df.drop(irr_cat_columns, axis=1)

# Assign attr class of remaining columns
nomi_cat_columns = np.intersect1d(nomi_cat_columns, df.columns)
boolean_columns = np.intersect1d(boolean_columns, df.columns)
nume_cat_columns = np.intersect1d(nume_cat_columns, df.columns)

nb_cat_columns = np.union1d(nomi_cat_columns, nume_cat_columns)
cat_columns = np.union1d(nb_cat_columns, boolean_columns)

# -- DROPPING IRRELEVANT CONT ATTRS --
# Encode nominal & numerical categorical columns to continuous
df = encode_all_cat_to_cont(df, encoding, nb_cat_columns, TARGET)

# Impute missing values for continuous & boolean columns
imputer = SimpleImputer()
imput_columns = np.union1d(boolean_columns, cont_columns)
df[imput_columns] = imputer.fit_transform(df[imput_columns])

# Remove irrelevant cont attrs using f table
cat_plus_cont_columns = np.union1d(cat_columns, cont_columns)
dico_f_df = f_table(df, cat_plus_cont_columns, TARGET,
                    'Continuous Attributes', False)
irr_cont_columns = dico_f_df.index[dico_f_df['f_p_good'] == False]
if (len(irr_cont_columns) > 0):
    print()
    print('Irrelevant Continuous & Encoded Categorical Columns')
    print(dico_f_df.loc[irr_cont_columns, :])
df = df.drop(irr_cont_columns, axis=1)

# -- DROPPING CORRELATED COLUMNS --
# Get correlated columns
corr_col_pairs = corr_pairs(df, TARGET, True)

# Get F-scores
check_corr_cols = set()
for col1, col2 in corr_col_pairs:
    check_corr_cols.add(col1)
    check_corr_cols.add(col2)

if len(check_corr_cols) > 0:
    dcor_f_df = f_table(df, check_corr_cols, TARGET, None, False)

    # Drop column with less F-score
    drop_corr_columns = list()
    for col1, col2 in corr_col_pairs:
        if dcor_f_df.loc[col1, 'f_val'] >= dcor_f_df.loc[col2, 'f_val']:
            drop_corr_columns.append(col2)
        else:
            drop_corr_columns.append(col1)

    print()
    print('Dropped Correlated Columns:')
    for col in drop_corr_columns:
        print(col)

    df = df.drop(drop_corr_columns, axis=1)

selected_columns = np.setdiff1d(df.columns, [TARGET])
fin_f_df = f_table(df, selected_columns, TARGET,
                   'Final Selected Attributes', True)

# Print list of columns to be copy pasted
print()
print('ALL SELECTED FEATURES:')
for index in fin_f_df.index:
    print(f'\'{index}\',')

# print()
# # Print list of columns to be copy pasted
# print('ALL SELECTED CAT COLUMNS')
# sel_cat_columns = np.intersect1d(fin_f_df.index, nb_cat_columns)
# for index in sel_cat_columns:
#     print(f'\'{index}\',')
